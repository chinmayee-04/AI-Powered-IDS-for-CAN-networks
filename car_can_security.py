# -*- coding: utf-8 -*-
"""car_can_security.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fVZw2-P1r2t5FKAw31WSdRqLwpG6J7jr
"""

#Data mmountingounting from drive


from google.colab import drive
drive.mount('/content/drive')

import os


base = '/content/drive/MyDrive/car-sec-project'


os.makedirs(base + '/data', exist_ok=True)
os.makedirs(base + '/models', exist_ok=True)
os.makedirs(base + '/notebooks', exist_ok=True)
print("Project structure created at:", base)

!pip -q install pandas numpy scikit-learn pyod matplotlib tensorflow

import nbformat

# Get the current notebook path
notebook_path = '/content/sample_notebook.ipynb' # This might need to be adjusted if not running in Google Colab

# In Colab, you can often access the current notebook path directly or iterate through the cells
# For simplicity, if running interactively in Colab, this code will print existing cells.
# This might need adjustment if the notebook is not saved or named 'sample_notebook.ipynb'

# A more robust way to get current notebook content in Colab is typically through the frontend API,
# but for a simple demonstration of extracting content, we'll assume a path.
# As an alternative, we can directly access the current notebook state (if available in the environment)

# --- Programmatic approach to list existing cell content ---
# This is a simplified way to get content if the notebook object is accessible.
# In a real Colab environment, accessing the current notebook's raw JSON might require a different approach.

# Placeholder for a method to get notebook cells from the environment
# In a typical Colab session, you might interact with the `google.colab.nbio` or similar if needed.
# For now, I'll print a message and then just list the Python cells if we were to simulate reading from a file.

print("--- Content of Code Cells ---")
# This loop demonstrates how you would iterate through cells if you had a loaded notebook object.
# Since direct access to the current notebook's internal representation isn't trivial for this agent,
# I'll provide a generic structure that would work if `notebook_cells` was available.
# For example, if you had loaded the notebook using `nbformat`:

# with open('your_notebook_name.ipynb', 'r') as f:
#     nb_content = nbformat.read(f, as_version=4)

# for cell in nb_content.cells:
#     if cell.cell_type == 'code':
#         print(f"\nCell Type: {cell.cell_type}\nContent:\n{cell.source}\n---")


# Since I cannot read the notebook file directly from the filesystem as an agent,
# and the `notebook_state` object does not contain all cell content in an easily iterable format,
# I will simulate printing the content of the cells provided in your context.
# If you save your notebook and provide its exact path, I can generate code to read it.

# This example is illustrative. To read *this specific running notebook's* content
# programmatically in Colab often requires more advanced techniques (e.g., using `!jupyter nbconvert --to script`)
# or saving the notebook and then reading it back.

print("You would typically see cell contents printed here if direct programmatic access to all current cells was simple for this environment.")
print("To get the full notebook content, please download the .ipynb file from 'File > Download > Download .ipynb'.")

import zipfile, os

data_path = '/content/drive/MyDrive/car-sec-project/data'
zip_path = os.path.join(data_path, 'In-Vehicle Network Intrusion Detection.zip')

# Extract files
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(data_path)

print("Files extracted to:", data_path)
os.listdir(data_path)

import os

data_path = '/content/drive/MyDrive/car-sec-project/data/In-Vehicle Network Intrusion Detection'
os.listdir(data_path)

import os

data_path = '/content/drive/MyDrive/car-sec-project/data/In-Vehicle Network Intrusion Detection/car_track_final_1st_release'
os.listdir(data_path)

import os

data_path = '/content/drive/MyDrive/car-sec-project/data/In-Vehicle Network Intrusion Detection/car_track_final_1st_train'
os.listdir(data_path)

import os

data_path = '/content/drive/MyDrive/car-sec-project/data/In-Vehicle Network Intrusion Detection/car_track_final_2nd_release'
os.listdir(data_path)

import os

data_path = '/content/drive/MyDrive/car-sec-project/data/In-Vehicle Network Intrusion Detection/car_track_final_2nd_train'
os.listdir(data_path)

import os

data_path = '/content/drive/MyDrive/car-sec-project/data/In-Vehicle Network Intrusion Detection/car_track_preliminary_release'
os.listdir(data_path)

import os

data_path = '/content/drive/MyDrive/car-sec-project/data/In-Vehicle Network Intrusion Detection/car_track_preliminary_train'
os.listdir(data_path)

import os, pandas as pd

# Base dataset path (your Google Drive folder)
base_path = "/content/drive/MyDrive/car-sec-project/data/In-Vehicle Network Intrusion Detection"

label_map = {
    'attack_free': 0,   # Normal
    'flooding': 1,      # DoS
    'fuzzy': 2,         # Random CAN IDs/data
    'replay': 3,        # Replay old frames
    'malfunction': 4    # Fault injection
}

def load_dataset(folder_path):
    data_frames = []
    for file in os.listdir(folder_path):
        if file.endswith('.csv'):
            fp = os.path.join(folder_path, file)
            df = pd.read_csv(fp)


            df.columns = [c.strip().upper().replace(" ", "_") for c in df.columns]


            fname_lower = file.lower()
            for key, val in label_map.items():
                if key in fname_lower:
                    df['ATTACK_LABEL'] = val
                    break

            data_frames.append(df)
            print(f"Loaded {file} → shape {df.shape}")
    return pd.concat(data_frames, ignore_index=True)

def clean_and_load(folder_path):
    data_frames = []
    for file in os.listdir(folder_path):
        if file.endswith('.csv'):
            fp = os.path.join(folder_path, file)
            df = pd.read_csv(fp, header=None)

            df = df.iloc[:, 0:4]
            df.columns = ['TIMESTAMP', 'CAN_ID', 'DLC', 'DATA']


            fname_lower = file.lower()
            for key, val in label_map.items():
                if key in fname_lower:
                    df['ATTACK_LABEL'] = val
                    break

            data_frames.append(df)
            print(f"Loaded {file} → shape {df.shape}")
    return pd.concat(data_frames, ignore_index=True)

train_path = os.path.join(base_path, "car_track_preliminary_train")

df_train = clean_and_load(train_path)

print("\nFinal merged shape:", df_train.shape)
print("\nColumns:", df_train.columns)
print("\nLabel distribution:\n", df_train['ATTACK_LABEL'].value_counts())
print("\nSample rows:\n", df_train.head(10))

import matplotlib.pyplot as plt

if 'CAN_ID' in df_train.columns:
    df_train['CAN_ID'].value_counts().head(20).plot(kind='bar')
    plt.title("Top 20 CAN IDs in Dataset")
    plt.xlabel("CAN ID")
    plt.ylabel("Frequency")
    plt.show()

df_train['ATTACK_LABEL'].value_counts().sort_index().plot(kind='bar')
plt.title("Attack Class Distribution (0=Normal, 1=Flooding, 2=Fuzzy, 3=Replay, 4=Malfunction)")
plt.xlabel("Attack Label")
plt.ylabel("Count")
plt.show()

train_path_final2 = os.path.join(base_path, "car_track_final_2nd_train")

df_final2 = clean_and_load(train_path_final2)

print("Final 2nd train shape:", df_final2.shape)
print("Label distribution:\n", df_final2['ATTACK_LABEL'].value_counts())


df_all = pd.concat([df_train, df_final2], ignore_index=True)

print("\nMerged full dataset shape:", df_all.shape)
print("\nLabel distribution across all attacks:\n", df_all['ATTACK_LABEL'].value_counts())

import numpy as np

# Copy dataset to avoid changes on original
df_ml = df_all.copy()

# Convert CAN_ID from hex to integer
def hex_to_int(x):
    try:
        return int(str(x), 16)
    except:
        return np.nan

df_ml['CAN_ID_INT'] = df_ml['CAN_ID'].apply(hex_to_int)

# Split DATA into 8 bytes
data_bytes = df_ml['DATA'].str.split(' ', expand=True)
data_bytes.columns = [f'DATA{i}' for i in range(data_bytes.shape[1])]

# Merge back
df_ml = pd.concat([df_ml, data_bytes], axis=1)

# Fill missing with 0
df_ml = df_ml.fillna('0')

# Convert all data byte columns to integers
for col in ['DLC'] + list(data_bytes.columns):
    df_ml[col] = df_ml[col].apply(lambda x: int(str(x), 16) if isinstance(x, str) else int(x))

# Final feature set
feature_cols = ['CAN_ID_INT', 'DLC'] + list(data_bytes.columns)
X = df_ml[feature_cols]
y = df_ml['ATTACK_LABEL']

print("Features shape:", X.shape)
print("Labels distribution:\n", y.value_counts())

from sklearn.model_selection import train_test_split

# Sample 200k rows for faster training (you can increase later)
df_sample = df_ml.sample(n=200000, random_state=42)

X = df_sample[feature_cols]
y = df_sample['ATTACK_LABEL']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, stratify=y, random_state=42
)

print("Train size:", X_train.shape, "Test size:", X_test.shape)

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(
    n_estimators=100,
    max_depth=20,
    n_jobs=-1,
    random_state=42
)

rf.fit(X_train, y_train)
print("Random Forest trained")

from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Predictions
y_pred = rf.predict(X_test)

# Report
print(classification_report(y_test, y_pred, digits=4))

# Confusion matrix
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=[0,1,2,3,4],
            yticklabels=[0,1,2,3,4])
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix - Random Forest Baseline")
plt.show()

import numpy as np
from sklearn.preprocessing import LabelEncoder

# Use subset for speed
df_seq = df_all.sample(n=200000, random_state=42)

# Convert CAN_ID to int
df_seq['CAN_ID_INT'] = df_seq['CAN_ID'].apply(lambda x: int(str(x), 16) if isinstance(x, str) else int(x))

# Split DATA into 8 bytes
data_bytes = df_seq['DATA'].str.split(' ', expand=True).fillna('00')
data_bytes.columns = [f'DATA{i}' for i in range(8)]

# Convert hex to int
for col in data_bytes.columns:
    data_bytes[col] = data_bytes[col].apply(lambda x: int(str(x), 16))

# Merge features
X_features = pd.concat([df_seq[['CAN_ID_INT','DLC']], data_bytes], axis=1)
y_labels = df_seq['ATTACK_LABEL'].values

# Normalize features
X_features = X_features.values / 255.0

SEQ_LEN = 20  # window size

def create_sequences(X, y, seq_len=SEQ_LEN):
    X_seq, y_seq = [], []
    for i in range(len(X) - seq_len):
        X_seq.append(X[i:i+seq_len])
        y_seq.append(y[i+seq_len])
    return np.array(X_seq), np.array(y_seq)

X_seq, y_seq = create_sequences(X_features, y_labels, SEQ_LEN)

print("Sequences shape:", X_seq.shape, y_seq.shape)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X_seq, y_seq, test_size=0.3, stratify=y_seq, random_state=42
)

print("Train:", X_train.shape, "Test:", X_test.shape)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout

num_classes = len(np.unique(y_seq))

model = Sequential([
    LSTM(64, input_shape=(SEQ_LEN, X_train.shape[2]), return_sequences=True),
    Dropout(0.2),
    LSTM(32),
    Dropout(0.2),
    Dense(64, activation='relu'),
    Dense(num_classes, activation='softmax')
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.summary()

history = model.fit(
    X_train, y_train,
    epochs=10,
    batch_size=128,
    validation_split=0.2,
    verbose=1
)

loss, acc = model.evaluate(X_test, y_test, verbose=0)
print(f"Test Accuracy: {acc:.4f}")

y_pred = np.argmax(model.predict(X_test), axis=1)

from sklearn.metrics import classification_report, confusion_matrix
print(classification_report(y_test, y_pred, digits=4))

import numpy as np

# Sample dataset for training speed
df_sample = df_all.sample(n=200000, random_state=42)

# Convert CAN_ID
df_sample['CAN_ID_INT'] = df_sample['CAN_ID'].apply(lambda x: int(str(x), 16) if isinstance(x, str) else int(x))

# Split DATA into 8 bytes
data_bytes = df_sample['DATA'].str.split(' ', expand=True).fillna('00')
data_bytes.columns = [f'DATA{i}' for i in range(8)]

for col in data_bytes.columns:
    data_bytes[col] = data_bytes[col].apply(lambda x: int(str(x), 16))

# Merge features
X = pd.concat([df_sample[['CAN_ID_INT','DLC']], data_bytes], axis=1)
y = df_sample['ATTACK_LABEL'].values

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, stratify=y, random_state=42
)

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_estimators=200, max_depth=20, n_jobs=-1, random_state=42)
rf.fit(X_train, y_train)
rf_pred = rf.predict(X_test)

from xgboost import XGBClassifier

xgb = XGBClassifier(
    n_estimators=200,
    max_depth=10,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    tree_method="hist",   # faster
    random_state=42
)

xgb.fit(X_train, y_train)
xgb_pred = xgb.predict(X_test)

from lightgbm import LGBMClassifier

lgbm = LGBMClassifier(
    n_estimators=200,
    max_depth=-1,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)

lgbm.fit(X_train, y_train)
lgbm_pred = lgbm.predict(X_test)

from sklearn.ensemble import GradientBoostingClassifier

gbdt = GradientBoostingClassifier(
    n_estimators=200,
    learning_rate=0.1,
    max_depth=5,
    random_state=42
)

gbdt.fit(X_train, y_train)
gbdt_pred = gbdt.predict(X_test)

from sklearn.metrics import classification_report

print("=== Random Forest ===")
print(classification_report(y_test, rf_pred, digits=4))

print("=== XGBoost ===")
print(classification_report(y_test, xgb_pred, digits=4))

print("=== LightGBM ===")
print(classification_report(y_test, lgbm_pred, digits=4))

print("=== Gradient Boosted DT ===")
print(classification_report(y_test, gbdt_pred, digits=4))

from sklearn.utils.class_weight import compute_class_weight
import numpy as np

classes = np.unique(y_train)
class_weights = compute_class_weight('balanced', classes=classes, y=y_train)
class_weights = dict(zip(classes, class_weights))

print("Class Weights:", class_weights)

# Add byte sum/mean
df_sample['BYTE_SUM'] = data_bytes.sum(axis=1)
df_sample['BYTE_MEAN'] = data_bytes.mean(axis=1)

# Message frequency (counts of each CAN_ID in sample)
id_counts = df_sample['CAN_ID_INT'].value_counts().to_dict()
df_sample['CAN_ID_FREQ'] = df_sample['CAN_ID_INT'].map(id_counts)

# Update feature set
X = pd.concat([df_sample[['CAN_ID_INT','DLC','BYTE_SUM','BYTE_MEAN','CAN_ID_FREQ']], data_bytes], axis=1)

import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical

# Sample for speed
df_cnn = df_all.sample(n=200000, random_state=42)

# CAN_ID to int
df_cnn['CAN_ID_INT'] = df_cnn['CAN_ID'].apply(lambda x: int(str(x), 16) if isinstance(x, str) else int(x))

# Split DATA into bytes
data_bytes = df_cnn['DATA'].str.split(' ', expand=True).fillna('00')
data_bytes.columns = [f'DATA{i}' for i in range(8)]
for col in data_bytes.columns:
    data_bytes[col] = data_bytes[col].apply(lambda x: int(str(x), 16))

# Final features (normalize 0–255)
X = pd.concat([df_cnn[['CAN_ID_INT','DLC']], data_bytes], axis=1).values
X = X / 255.0

y = df_cnn['ATTACK_LABEL'].values
num_classes = len(np.unique(y))

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)

# Reshape for CNN (samples, timesteps, features=1)
X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))

# One-hot encode labels
y_train_cat = to_categorical(y_train, num_classes)
y_test_cat = to_categorical(y_test, num_classes)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization

model = Sequential([
    Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)),
    BatchNormalization(),
    MaxPooling1D(pool_size=2),

    Conv1D(128, kernel_size=3, activation='relu'),
    BatchNormalization(),
    MaxPooling1D(pool_size=2),

    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.3),
    Dense(num_classes, activation='softmax')
])

model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

model.summary()

history = model.fit(
    X_train, y_train_cat,
    epochs=15,
    batch_size=256,
    validation_split=0.2,
    verbose=1
)

loss, acc = model.evaluate(X_test, y_test_cat, verbose=0)
print(f"Test Accuracy: {acc:.4f}")

y_pred = np.argmax(model.predict(X_test), axis=1)

from sklearn.metrics import classification_report, confusion_matrix
print(classification_report(y_test, y_pred, digits=4))

import numpy as np
import pandas as pd

# Sample for training speed
df_hybrid = df_all.sample(n=200000, random_state=42)

# Convert CAN_ID
df_hybrid['CAN_ID_INT'] = df_hybrid['CAN_ID'].apply(lambda x: int(str(x), 16) if isinstance(x, str) else int(x))

# Split DATA into bytes
data_bytes = df_hybrid['DATA'].str.split(' ', expand=True).fillna('00')
data_bytes.columns = [f'DATA{i}' for i in range(8)]
for col in data_bytes.columns:
    data_bytes[col] = data_bytes[col].apply(lambda x: int(str(x), 16))

# Time delta (sorted by timestamp)
df_hybrid = df_hybrid.sort_values("TIMESTAMP")
df_hybrid['TIME_DELTA'] = df_hybrid['TIMESTAMP'].diff().fillna(0)

# CAN_ID frequency
id_counts = df_hybrid['CAN_ID_INT'].value_counts().to_dict()
df_hybrid['CAN_ID_FREQ'] = df_hybrid['CAN_ID_INT'].map(id_counts)

# Byte entropy
import math
def byte_entropy(data_str):
    bytes_list = data_str.split()
    if not bytes_list:
        return 0
    counts = {}
    for b in bytes_list:
        counts[b] = counts.get(b, 0) + 1
    probs = [c/len(bytes_list) for c in counts.values()]
    return -sum(p * math.log2(p) for p in probs)
df_hybrid['BYTE_ENTROPY'] = df_hybrid['DATA'].apply(byte_entropy)

# Rolling stats
df_hybrid['BYTE_SUM'] = data_bytes.sum(axis=1)
df_hybrid['BYTE_MEAN'] = data_bytes.mean(axis=1)
df_hybrid['BYTE_STD'] = data_bytes.std(axis=1)

# Final feature matrix
X = pd.concat([
    df_hybrid[['CAN_ID_INT','DLC','TIME_DELTA','CAN_ID_FREQ','BYTE_ENTROPY','BYTE_SUM','BYTE_MEAN','BYTE_STD']],
    data_bytes
], axis=1).values

y = df_hybrid['ATTACK_LABEL'].values

SEQ_LEN = 20

def create_sequences(X, y, seq_len=SEQ_LEN):
    X_seq, y_seq = [], []
    for i in range(len(X) - seq_len):
        X_seq.append(X[i:i+seq_len])
        y_seq.append(y[i+seq_len])
    return np.array(X_seq), np.array(y_seq)

X_seq, y_seq = create_sequences(X, y, SEQ_LEN)

print("Sequences shape:", X_seq.shape, y_seq.shape)

from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical

num_classes = len(np.unique(y_seq))

X_train, X_test, y_train, y_test = train_test_split(
    X_seq, y_seq, test_size=0.3, stratify=y_seq, random_state=42
)

# One-hot labels
y_train_cat = to_categorical(y_train, num_classes)
y_test_cat = to_categorical(y_test, num_classes)

print("Train:", X_train.shape, "Test:", X_test.shape)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout, BatchNormalization, Flatten

model = Sequential([
    Conv1D(64, kernel_size=3, activation='relu', input_shape=(SEQ_LEN, X_train.shape[2])),
    BatchNormalization(),
    MaxPooling1D(pool_size=2),

    Conv1D(128, kernel_size=3, activation='relu'),
    BatchNormalization(),
    MaxPooling1D(pool_size=2),

    LSTM(64, return_sequences=False),
    Dropout(0.3),

    Dense(128, activation='relu'),
    Dropout(0.3),
    Dense(num_classes, activation='softmax')
])

model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

model.summary()

history = model.fit(
    X_train, y_train_cat,
    epochs=15,
    batch_size=128,
    validation_split=0.2,
    verbose=1
)

loss, acc = model.evaluate(X_test, y_test_cat, verbose=0)
print(f"Test Accuracy: {acc:.4f}")

y_pred = np.argmax(model.predict(X_test), axis=1)

from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred, digits=4))

from sklearn.utils.class_weight import compute_class_weight
import numpy as np

classes = np.unique(y_train)
class_weights = compute_class_weight('balanced', classes=classes, y=y_train)
class_weights = dict(zip(classes, class_weights))

history = model.fit(
    X_train, y_train_cat,
    epochs=20,
    batch_size=128,
    validation_split=0.2,
    class_weight=class_weights,
    verbose=1
)

from sklearn.utils.class_weight import compute_class_weight
import numpy as np

classes = np.unique(y_train)
class_weights = compute_class_weight('balanced', classes=classes, y=y_train)
class_weights = dict(zip(classes, class_weights))

history = model.fit(
    X_train, y_train_cat,
    epochs=20,
    batch_size=128,
    validation_split=0.2,
    class_weight=class_weights,
    verbose=1
)

import tensorflow as tf
from tensorflow.keras.layers import Dense, Dropout, LayerNormalization, Input, Flatten
from tensorflow.keras.layers import MultiHeadAttention, GlobalAveragePooling1D, Embedding
from tensorflow.keras.models import Model

SEQ_LEN = 20
FEATURE_DIM = X_train.shape[2]
NUM_CLASSES = len(np.unique(y_train))

# Transformer Encoder Block
def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):
    # Multi-head self-attention
    x = MultiHeadAttention(num_heads=num_heads, key_dim=head_size)(inputs, inputs)
    x = Dropout(dropout)(x)
    x = LayerNormalization(epsilon=1e-6)(x + inputs)

    # Feed-forward layer
    x_ff = Dense(ff_dim, activation="relu")(x)
    x_ff = Dropout(dropout)(x_ff)
    x_ff = Dense(inputs.shape[-1])(x_ff)
    x = LayerNormalization(epsilon=1e-6)(x + x_ff)
    return x

# Input
inputs = Input(shape=(SEQ_LEN, FEATURE_DIM))

# 2 Transformer blocks
x = transformer_encoder(inputs, head_size=64, num_heads=4, ff_dim=128, dropout=0.3)
x = transformer_encoder(x, head_size=64, num_heads=4, ff_dim=128, dropout=0.3)

# Global pooling
x = GlobalAveragePooling1D()(x)

# Dense layers
x = Dense(128, activation="relu")(x)
x = Dropout(0.3)(x)
outputs = Dense(NUM_CLASSES, activation="softmax")(x)

model = Model(inputs, outputs)

model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
    loss="categorical_crossentropy",
    metrics=["accuracy"]
)

model.summary()

callback_list = [
    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3),
    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
]

history = model.fit(
    X_train, y_train_cat,
    epochs=20,
    batch_size=128,
    validation_split=0.2,
    class_weight=class_weights,  # from Phase 6 balancing
    callbacks=callback_list,
    verbose=1
)

loss, acc = model.evaluate(X_test, y_test_cat, verbose=0)
print(f"Test Accuracy: {acc:.4f}")

y_pred = np.argmax(model.predict(X_test), axis=1)

from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred, digits=4))

# After defining CNN+LSTM model
model_lstm = model  # rename it before training

history_lstm = model_lstm.fit(
    X_train, y_train_cat,
    epochs=20,
    batch_size=128,
    validation_split=0.2,
    verbose=1
)

# After defining Transformer model
model_trans = model  # rename it before training

history_trans = model_trans.fit(
    X_train, y_train_cat,
    epochs=25,
    batch_size=128,
    validation_split=0.2,
    verbose=1
)

import numpy as np
from sklearn.metrics import classification_report, accuracy_score

# Get probability predictions from both models
y_pred_proba_lstm = model_lstm.predict(X_test)
y_pred_proba_trans = model_trans.predict(X_test)

# Average probabilities
y_pred_proba_fusion = (y_pred_proba_lstm + y_pred_proba_trans) / 2.0

# Final prediction = class with highest probability
y_pred_fusion = np.argmax(y_pred_proba_fusion, axis=1)

# Evaluate
print("Fusion Accuracy:", accuracy_score(y_test, y_pred_fusion))
print(classification_report(y_test, y_pred_fusion, digits=4))

"""PREDICTIONS"""

import numpy as np

# Pick 10 random samples from test data
idx = np.random.choice(len(X_test), 10, replace=False)
X_sample = X_test[idx]
y_true_sample = y_test[idx]

# Get probability predictions
proba_lstm = model_lstm.predict(X_sample)
proba_trans = model_trans.predict(X_sample)

# Fusion: average of both
proba_fusion = (proba_lstm + proba_trans) / 2.0

# Final predictions
y_pred_sample = np.argmax(proba_fusion, axis=1)

label_map = {
    0: "Normal",
    1: "Flooding Attack",
    2: "Fuzzy Attack",
    3: "Replay Attack",
    4: "Malfunction Attack"
}

print("\n Sample Predictions:")
for i in range(len(y_pred_sample)):
    print(f"Message {i+1}: True = {label_map[y_true_sample[i]]}, Predicted = {label_map[y_pred_sample[i]]}")

import pandas as pd


sample_normal = X_test[y_test == 0][0].reshape(1, -1, X_test.shape[2])
sample_flooding = X_test[y_test == 1][0].reshape(1, -1, X_test.shape[2])
sample_fuzzy = X_test[y_test == 2][0].reshape(1, -1, X_test.shape[2])
sample_replay = X_test[y_test == 3][0].reshape(1, -1, X_test.shape[2])
sample_malfunction = X_test[y_test == 4][0].reshape(1, -1, X_test.shape[2])

targeted_samples = [
    ("Normal", sample_normal, 0),
    ("Flooding", sample_flooding, 1),
    ("Fuzzy", sample_fuzzy, 2),
    ("Replay", sample_replay, 3),
    ("Malfunction", sample_malfunction, 4),
]

def predict_fusion(sample):
    proba_lstm = model_lstm.predict(sample)
    proba_trans = model_trans.predict(sample)
    proba_fusion = (proba_lstm + proba_trans) / 2.0
    return np.argmax(proba_fusion, axis=1)[0]

print("\n Targeted Predictions:")
for name, sample, true_label in targeted_samples:
    pred = predict_fusion(sample)
    print(f"True: {name:12} | Predicted: {label_map[pred]}")

import tensorflow as tf
from tensorflow import keras


model = keras.Sequential([
    keras.layers.Dense(64, activation='relu', input_shape=(10,)),
    keras.layers.Dense(1, activation='sigmoid')
])


model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])



model.save('my_model.h5')

print("Model saved successfully as my_model.h5")

